{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_collate(data):\n",
    "    (obs_seq_list, pred_seq_list, obs_seq_rel_list, pred_seq_rel_list,\n",
    "     non_linear_ped_list, loss_mask_list) = zip(*data)\n",
    "\n",
    "    _len = [len(seq) for seq in obs_seq_list]\n",
    "    cum_start_idx = [0] + np.cumsum(_len).tolist()\n",
    "    seq_start_end = [[start, end]\n",
    "                     for start, end in zip(cum_start_idx, cum_start_idx[1:])]\n",
    "\n",
    "    # Data format: batch, input_size, seq_len\n",
    "    # LSTM input format: seq_len, batch, input_size\n",
    "    obs_traj = torch.cat(obs_seq_list, dim=0).permute(2, 0, 1)\n",
    "    pred_traj = torch.cat(pred_seq_list, dim=0).permute(2, 0, 1)\n",
    "    obs_traj_rel = torch.cat(obs_seq_rel_list, dim=0).permute(2, 0, 1)\n",
    "    pred_traj_rel = torch.cat(pred_seq_rel_list, dim=0).permute(2, 0, 1)\n",
    "    non_linear_ped = torch.cat(non_linear_ped_list)\n",
    "    loss_mask = torch.cat(loss_mask_list, dim=0)\n",
    "    seq_start_end = torch.LongTensor(seq_start_end)\n",
    "    out = [\n",
    "        obs_traj, pred_traj, obs_traj_rel, pred_traj_rel, non_linear_ped,\n",
    "        loss_mask, seq_start_end\n",
    "    ]\n",
    "\n",
    "    return tuple(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(_path, delim='\\t'):\n",
    "    data = []\n",
    "    if delim == 'tab':\n",
    "        delim = '\\t'\n",
    "    elif delim == 'space':\n",
    "        delim = ' '\n",
    "    with open(_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(delim)\n",
    "            line = [float(i) for i in line]\n",
    "            data.append(line)\n",
    "    return np.asarray(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_fit(traj, traj_len, threshold):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - traj: Numpy array of shape (2, traj_len)\n",
    "    - traj_len: Len of trajectory\n",
    "    - threshold: Minimum error to be considered for non linear traj\n",
    "    Output:\n",
    "    - int: 1 -> Non Linear 0-> Linear\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, traj_len - 1, traj_len)\n",
    "    res_x = np.polyfit(t, traj[0, -traj_len:], 2, full=True)[1]\n",
    "    res_y = np.polyfit(t, traj[1, -traj_len:], 2, full=True)[1]\n",
    "    if res_x + res_y >= threshold:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    \"\"\"Dataloder for the Trajectory datasets\"\"\"\n",
    "    def __init__(\n",
    "        self, file_path, obs_len=8, pred_len=12, skip=1, threshold=0.002,\n",
    "        min_ped=0, delim='\\t'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - data_dir: Directory containing dataset files in the format\n",
    "        <frame_id> <ped_id> <x> <y>\n",
    "        - obs_len: Number of time-steps in input trajectories\n",
    "        - pred_len: Number of time-steps in output trajectories\n",
    "        - skip: Number of frames to skip while making the dataset\n",
    "        - threshold: Minimum error to be considered for non linear traj\n",
    "        when using a linear predictor\n",
    "        - min_ped: Minimum number of pedestrians that should be in a seqeunce\n",
    "        - delim: Delimiter in the dataset files\n",
    "        \"\"\"\n",
    "        super(TrajectoryDataset, self).__init__()\n",
    "\n",
    "        self.file_path = file_path\n",
    "        self.obs_len = obs_len\n",
    "        self.pred_len = pred_len\n",
    "        self.skip = skip\n",
    "        self.seq_len = self.obs_len + self.pred_len\n",
    "        self.delim = delim\n",
    "\n",
    "        \n",
    "        num_peds_in_seq = []\n",
    "        seq_list = []\n",
    "        seq_list_rel = []\n",
    "        loss_mask_list = []\n",
    "        non_linear_ped = []\n",
    "        \n",
    "        data = read_file(self.file_path, delim)\n",
    "        frames = np.unique(data[:, 0]).tolist()\n",
    "        frame_data = []\n",
    "\n",
    "        for frame in tqdm(frames):\n",
    "            frame_data.append(data[frame == data[:, 0], :])\n",
    "\n",
    "        num_sequences = int(\n",
    "            math.ceil((len(frames) - self.seq_len + 1) / skip))\n",
    "        \n",
    "        for idx in tqdm(range(0, num_sequences * self.skip + 1, skip)):\n",
    "            curr_seq_data = np.concatenate(\n",
    "                frame_data[idx:idx + self.seq_len], axis=0)\n",
    "            peds_in_curr_seq = np.unique(curr_seq_data[:, 1])\n",
    "            curr_seq_rel = np.zeros((len(peds_in_curr_seq), 2,\n",
    "                                     self.seq_len))\n",
    "            curr_seq = np.zeros((len(peds_in_curr_seq), 2, self.seq_len))\n",
    "            curr_loss_mask = np.zeros((len(peds_in_curr_seq),\n",
    "                                       self.seq_len))\n",
    "            num_peds_considered = 0\n",
    "            _non_linear_ped = []\n",
    "            for _, ped_id in enumerate(peds_in_curr_seq):\n",
    "                curr_ped_seq = curr_seq_data[curr_seq_data[:, 1] ==\n",
    "                                             ped_id, :]\n",
    "                curr_ped_seq = np.around(curr_ped_seq, decimals=4)\n",
    "                pad_front = frames.index(curr_ped_seq[0, 0]) - idx\n",
    "                pad_end = frames.index(curr_ped_seq[-1, 0]) - idx + 1\n",
    "                if pad_end - pad_front != self.seq_len:\n",
    "                    continue\n",
    "                curr_ped_seq = np.transpose(curr_ped_seq[:, 2:])\n",
    "                curr_ped_seq = curr_ped_seq\n",
    "                # Make coordinates relative\n",
    "                rel_curr_ped_seq = np.zeros(curr_ped_seq.shape)\n",
    "                rel_curr_ped_seq[:, 1:] = \\\n",
    "                    curr_ped_seq[:, 1:] - curr_ped_seq[:, :-1]\n",
    "                _idx = num_peds_considered\n",
    "                curr_seq[_idx, :, pad_front:pad_end] = curr_ped_seq\n",
    "                curr_seq_rel[_idx, :, pad_front:pad_end] = rel_curr_ped_seq\n",
    "                # Linear vs Non-Linear Trajectory\n",
    "                _non_linear_ped.append(\n",
    "                    poly_fit(curr_ped_seq, pred_len, threshold))\n",
    "                curr_loss_mask[_idx, pad_front:pad_end] = 1\n",
    "                num_peds_considered += 1\n",
    "            if num_peds_considered > min_ped:\n",
    "                non_linear_ped += _non_linear_ped\n",
    "                num_peds_in_seq.append(num_peds_considered)\n",
    "                loss_mask_list.append(curr_loss_mask[:num_peds_considered])\n",
    "                seq_list.append(curr_seq[:num_peds_considered])\n",
    "                seq_list_rel.append(curr_seq_rel[:num_peds_considered])\n",
    "            \n",
    "\n",
    "        self.num_seq = len(seq_list)\n",
    "        seq_list = np.concatenate(seq_list, axis=0)\n",
    "        seq_list_rel = np.concatenate(seq_list_rel, axis=0)\n",
    "        loss_mask_list = np.concatenate(loss_mask_list, axis=0)\n",
    "        non_linear_ped = np.asarray(non_linear_ped)\n",
    "\n",
    "        # Convert numpy -> Torch Tensor\n",
    "        self.obs_traj = torch.from_numpy(\n",
    "            seq_list[:, :, :self.obs_len]).type(torch.float)\n",
    "        self.pred_traj = torch.from_numpy(\n",
    "            seq_list[:, :, self.obs_len:]).type(torch.float)\n",
    "        self.obs_traj_rel = torch.from_numpy(\n",
    "            seq_list_rel[:, :, :self.obs_len]).type(torch.float)\n",
    "        self.pred_traj_rel = torch.from_numpy(\n",
    "            seq_list_rel[:, :, self.obs_len:]).type(torch.float)\n",
    "        self.loss_mask = torch.from_numpy(loss_mask_list).type(torch.float)\n",
    "        self.non_linear_ped = torch.from_numpy(non_linear_ped).type(torch.float)\n",
    "        cum_start_idx = [0] + np.cumsum(num_peds_in_seq).tolist()\n",
    "        self.seq_start_end = [\n",
    "            (start, end)\n",
    "            for start, end in zip(cum_start_idx, cum_start_idx[1:])\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_seq\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start, end = self.seq_start_end[index]\n",
    "        out = [\n",
    "            self.obs_traj[start:end, :], self.pred_traj[start:end, :],\n",
    "            self.obs_traj_rel[start:end, :], self.pred_traj_rel[start:end, :],\n",
    "            self.non_linear_ped[start:end], self.loss_mask[start:end, :]\n",
    "        ]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14164/14164 [00:01<00:00, 10263.56it/s]\n",
      "100%|██████████| 14146/14146 [00:19<00:00, 712.45it/s]\n",
      "100%|██████████| 8346/8346 [00:00<00:00, 18981.29it/s]\n",
      "100%|██████████| 8328/8328 [00:07<00:00, 1073.69it/s]\n",
      "100%|██████████| 50274/50274 [00:16<00:00, 3067.80it/s]\n",
      "100%|██████████| 50256/50256 [03:33<00:00, 235.70it/s]\n"
     ]
    }
   ],
   "source": [
    "types = [\"test\",\"val\",\"train\"]\n",
    "\n",
    "for TYPE in types:\n",
    "    train_obj = TrajectoryDataset(f\"output\\\\German1_SPLIT\\\\{TYPE}.txt\",obs_len=8, pred_len=12)\n",
    "    with open(f\"output\\\\German1_PKL\\\\{TYPE}.pkl\",\"wb\") as file:\n",
    "        pickle.dump(train_obj, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
